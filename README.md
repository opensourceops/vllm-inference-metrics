# vllm-inference-metrics
Observability stack for vLLM inference deployments with metrics collection, sanitization, and storage using Fluent Bit and Parseable
